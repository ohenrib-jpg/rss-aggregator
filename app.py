import os
import json
import datetime
import requests
from flask import Flask, request, jsonify, send_from_directory
from bs4 import BeautifulSoup
import re
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.platypus.flowables import Spacer
import matplotlib.pyplot as plt
import io
import base64

app = Flask(__name__)
REPORTS_DIR = os.path.join(os.path.dirname(__file__), 'reports')
os.makedirs(REPORTS_DIR, exist_ok=True)

# Service de recherche web avanc√©
class AdvancedWebResearch:
    def __init__(self):
        self.trusted_sources = [
            'reuters.com', 'apnews.com', 'bbc.com', 'theguardian.com',
            'lemonde.fr', 'liberation.fr', 'figaro.fr', 'france24.com'
        ]
    
    def search_contextual_info(self, article_title, themes):
        """Recherche des informations contextuelles sur le web"""
        try:
            # Recherche sur des sources fiables
            search_terms = self.build_search_query(article_title, themes)
            contextual_data = []
            
            for source in self.trusted_sources[:2]:  # Limiter pour performance
                try:
                    data = self.search_on_source(source, search_terms)
                    if data:
                        contextual_data.append(data)
                except Exception as e:
                    print(f"‚ùå Erreur recherche {source}: {e}")
            
            # CORRECTION : utiliser article_title au lieu de original_title
            return self.analyze_contextual_data(contextual_data, article_title)
            
        except Exception as e:
            print(f"‚ùå Erreur recherche contextuelle: {e}")
            return None
    
    def build_search_query(self, title, themes):
        """Construit une requ√™te de recherche optimis√©e"""
        # Extraire les entit√©s nomm√©es
        entities = self.extract_entities(title)
        
        # CORRECTION : g√©rer les th√®mes comme liste de strings
        theme_keywords = ''
        if themes:
            if isinstance(themes, list):
                theme_keywords = ' OR '.join(str(t) for t in themes[:3])
            else:
                theme_keywords = str(themes)
        
        query = f"({title}) {theme_keywords}"
        if entities:
            query += f" {' '.join(entities)}"
        
        return query
    
    def extract_entities(self, text):
        """Extraction basique d'entit√©s nomm√©es"""
        # Patterns pour les entit√©s g√©opolitiques
        patterns = {
            'pays': r'\b(France|Allemagne|√âtats-Unis|USA|China|Chine|Russie|UK|Royaume-Uni|Ukraine|Israel|Palestine)\b',
            'organisations': r'\b(ONU|OTAN|UE|Union Europ√©enne|UN|NATO|OMS|WHO)\b',
            'personnes': r'\b(Poutine|Zelensky|Macron|Biden|Xi|Merkel|Scholz)\b'
        }
        
        entities = []
        for category, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            entities.extend(matches)
        
        return entities
    
    def search_on_source(self, source, query):
        """Recherche sur une source sp√©cifique (simul√©e pour l'instant)"""
        # Impl√©mentation simul√©e - √† remplacer par une vraie recherche
        return {
            'source': source,
            'title': f"Article contextuel sur {query}",
            'content': f"Informations contextuelles r√©cup√©r√©es de {source} concernant {query}",
            'sentiment': 'neutral',
            'date': datetime.datetime.now().isoformat()
        }
    
    def analyze_contextual_data(self, contextual_data, article_title):
        """Analyse les donn√©es contextuelles pour d√©tecter les divergences"""
        if not contextual_data:
            return None
        
        # Analyse de coh√©rence
        sentiment_scores = []
        key_facts = []
        
        for data in contextual_data:
            sentiment = self.analyze_sentiment(data['content'])
            sentiment_scores.append(sentiment)
            key_facts.extend(self.extract_key_facts(data['content']))
        
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
        
        return {
            'sources_consult√©es': len(contextual_data),
            'sentiment_moyen': avg_sentiment,
            'faits_cles': list(set(key_facts))[:5],  # D√©dupliquer et limiter
            'coherence': self.calculate_coherence(sentiment_scores),
            'recommendations': self.generate_recommendations(avg_sentiment, key_facts)
        }
    
    def analyze_sentiment(self, text):
        """Analyse de sentiment simplifi√©e"""
        positive_words = ['accord', 'paix', 'progr√®s', 'succ√®s', 'coop√©ration', 'dialogue']
        negative_words = ['conflit', 'crise', 'tension', 'sanction', 'violence', 'protestation']
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        total = positive_count + negative_count
        if total == 0:
            return 0
        
        return (positive_count - negative_count) / total
    
    def extract_key_facts(self, text):
        """Extraction de faits cl√©s"""
        facts = []
        
        # Patterns pour les faits importants
        fact_patterns = [
            r'accord sur\s+([^.,]+)',
            r'sanctions?\s+contre\s+([^.,]+)',
            r'crise\s+(?:au|en)\s+([^.,]+)',
            r'n√©gociations?\s+(?:√†|en)\s+([^.,]+)'
        ]
        
        for pattern in fact_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            facts.extend(matches)
        
        return facts
    
    def calculate_coherence(self, sentiment_scores):
        """Calcule la coh√©rence entre les sources"""
        if len(sentiment_scores) < 2:
            return 1.0
        
        variance = sum((score - sum(sentiment_scores)/len(sentiment_scores))**2 for score in sentiment_scores)
        return max(0, 1 - variance)
    
    def generate_recommendations(self, sentiment, key_facts):
        """G√©n√®re des recommandations bas√©es sur l'analyse"""
        recommendations = []
        
        if abs(sentiment) > 0.3:
            recommendations.append("√âcart sentiment d√©tect√© - v√©rification recommand√©e")
        
        if key_facts:
            recommendations.append(f"Faits contextuels identifi√©s: {', '.join(key_facts[:3])}")
        
        if len(recommendations) == 0:
            recommendations.append("Coh√©rence g√©n√©rale avec le contexte m√©diatique")
        
        return recommendations

# Analyseur IA avanc√© avec raisonnement
class AdvancedIAAnalyzer:
    def __init__(self):
        self.web_research = AdvancedWebResearch()
        self.analysis_framework = {
            'g√©opolitique': self.analyze_geopolitical_context,
            '√©conomique': self.analyze_economic_context,
            'social': self.analyze_social_context,
            'environnement': self.analyze_environmental_context
        }
    
    def perform_deep_analysis(self, article, themes):
        """Analyse approfondie avec raisonnement"""
        print(f"üß† Analyse approfondie: {article.get('title', '')[:50]}...")
        
        try:
            # CORRECTION : extraire les noms des th√®mes
            theme_names = []
            if themes:
                if isinstance(themes, list):
                    theme_names = [t.get('name', str(t)) if isinstance(t, dict) else str(t) for t in themes]
                else:
                    theme_names = [str(themes)]
            
            # 1. Analyse contextuelle avanc√©e
            contextual_analysis = self.analyze_advanced_context(article, theme_names)
            
            # 2. Recherche web pour v√©rification
            web_research = self.web_research.search_contextual_info(
                article.get('title', ''), 
                theme_names
            )
            
            # 3. Analyse th√©matique sp√©cialis√©e
            thematic_analysis = self.analyze_thematic_context(article, theme_names)
            
            # 4. D√©tection de biais et v√©rification
            bias_analysis = self.analyze_biases(article, contextual_analysis, web_research)
            
            # 5. Synth√®se et recommandations
            final_analysis = self.synthesize_analysis(
                article, 
                contextual_analysis, 
                web_research, 
                thematic_analysis, 
                bias_analysis
            )
            
            return final_analysis
            
        except Exception as e:
            print(f"‚ùå Erreur analyse approfondie: {e}")
            import traceback
            traceback.print_exc()
            
            # Retourner une analyse par d√©faut en cas d'erreur
            sentiment = article.get('sentiment', {})
            return {
                'score_original': sentiment.get('score', 0),
                'score_corrig√©': sentiment.get('score', 0),
                'confiance': 0.3,
                'analyse_contextuelle': {},
                'recherche_web': None,
                'analyse_thematique': {},
                'analyse_biases': {'biais_d√©tect√©s': [], 'score_credibilite': 0.5},
                'recommandations_globales': ['Erreur lors de l\'analyse approfondie']
            }
    
    def analyze_advanced_context(self, article, themes):
        """Analyse contextuelle avanc√©e"""
        title = article.get('title', '')
        content = article.get('content', '')
        full_text = f"{title} {content}"
        
        analysis = {
            'urgence': self.assess_urgency(full_text),
            'port√©e': self.assess_scope(full_text),
            'impact': self.assess_impact(full_text, themes),
            'nouveaut√©': self.assess_novelty(full_text),
            'controverses': self.detect_controversies(full_text)
        }
        
        return analysis
    
    def assess_urgency(self, text):
        """√âvalue l'urgence de l'information"""
        urgent_indicators = ['urgence', 'crise', 'imm√©diat', 'drame', 'catastrophe', 'attaque']
        text_lower = text.lower()
        
        urgency_score = sum(1 for indicator in urgent_indicators if indicator in text_lower)
        return min(1.0, urgency_score / 3)
    
    def assess_scope(self, text):
        """√âvalue la port√©e g√©ographique"""
        scopes = {
            'local': ['ville', 'r√©gion', 'local', 'municipal'],
            'national': ['France', 'pays', 'national', 'gouvernement'],
            'international': ['monde', 'international', 'ONU', 'OTAN', 'UE']
        }
        
        text_lower = text.lower()
        scope_scores = {}
        
        for scope, indicators in scopes.items():
            score = sum(1 for indicator in indicators if indicator in text_lower)
            scope_scores[scope] = score
        
        return max(scope_scores, key=scope_scores.get) if scope_scores else 'local'
    
    def assess_impact(self, text, themes):
        """√âvalue l'impact potentiel"""
        high_impact_indicators = [
            'crise', 'r√©cession', 'guerre', 'sanctions', 'accord historique',
            'rupture', 'r√©volution', 'transition'
        ]
        
        text_lower = text.lower()
        impact_score = sum(1 for indicator in high_impact_indicators if indicator in text_lower)
        
        # Pond√©ration par th√®me
        theme_weights = {
            'conflit': 1.5, '√©conomie': 1.3, 'diplomatie': 1.2,
            'environnement': 1.1, 'social': 1.0
        }
        
        theme_weight = 1.0
        for theme in themes:
            theme_lower = str(theme).lower() if theme else ''
            if theme_lower in theme_weights:
                theme_weight = max(theme_weight, theme_weights[theme_lower])
        
        return min(1.0, (impact_score / 5) * theme_weight)
    
    def assess_novelty(self, text):
        """√âvalue la nouveaut√© de l'information"""
        novel_indicators = [
            'nouveau', 'premier', 'historique', 'inaugural', 'innovation',
            'r√©volutionnaire', 'changement', 'r√©forme'
        ]
        
        text_lower = text.lower()
        novelty_score = sum(1 for indicator in novel_indicators if indicator in text_lower)
        return min(1.0, novelty_score / 4)
    
    def detect_controversies(self, text):
        """D√©tecte les controverses potentielles"""
        controversy_indicators = [
            'pol√©mique', 'controvers√©', 'd√©bat', 'opposition', 'critique',
            'protestation', 'manifestation', 'conflit d\'int√©r√™t'
        ]
        
        text_lower = text.lower()
        controversies = []
        
        for indicator in controversy_indicators:
            if indicator in text_lower:
                # Trouver le contexte autour de l'indicateur
                start = max(0, text_lower.find(indicator) - 50)
                end = min(len(text), text_lower.find(indicator) + len(indicator) + 50)
                context = text[start:end].strip()
                controversies.append(f"{indicator}: {context}")
        
        return controversies
    
    def analyze_thematic_context(self, article, themes):
        """Analyse contextuelle par th√®me"""
        thematic_analysis = {}
        
        # CORRECTION : s'assurer que themes est une liste de cha√Ænes
        theme_list = []
        if themes:
            if isinstance(themes, list):
                theme_list = [str(t) for t in themes]
            else:
                theme_list = [str(themes)]
        
        for theme in theme_list:
            theme_lower = theme.lower() if theme else ''
            
            if theme_lower in self.analysis_framework:
                try:
                    analysis = self.analysis_framework[theme_lower](article)
                    thematic_analysis[theme] = analysis
                except Exception as e:
                    print(f"‚ùå Erreur analyse th√®me {theme}: {e}")
        
        return thematic_analysis

    def analyze_economic_context(self, article):
        """Analyse contextuelle √©conomique"""
        text = f"{article.get('title', '')} {article.get('content', '')}"
        
        return {
            'indicateurs': self.extract_economic_indicators(text),
            'secteurs': self.identify_economic_sectors(text),
            'impact_economique': self.assess_economic_impact(text),
            'tendances': self.detect_economic_trends(text),
            'recommandations': self.generate_economic_recommendations(text)
        }

    def extract_economic_indicators(self, text):
        """Extrait les indicateurs √©conomiques mentionn√©s"""
        indicators = {
            'macro√©conomiques': {
                'patterns': [
                    r'PIB\s*(?:de|du|\s)([^.,;]+)',
                    r'croissance\s+√©conomique\s+de\s+([\d,]+)%',
                    r'inflation\s+de\s+([\d,]+)%',
                    r'ch√¥mage\s+de\s+([\d,]+)%',
                    r'dette\s+publique\s+de\s+([\d,]+)',
                    r'd√©ficit\s+budg√©taire\s+de\s+([\d,]+)'
                ],
                'matches': []
            },
            'financiers': {
                'patterns': [
                    r'march√©s?\s+boursiers?\s+([^.,;]+)',
                    r'indice\s+([A-Z]+)\s+([\d,]+)',
                    r'euro\s+([\d,]+)\s+dollars?',
                    r'dollar\s+([\d,]+)\s+euros?',
                    r'taux\s+directeur\s+([^.,;]+)',
                    r'banque\s+centrale\s+([^.,;]+)'
                ],
                'matches': []
            },
            'commerciaux': {
                'patterns': [
                    r'commerce\s+ext√©rieur\s+([^.,;]+)',
                    r'exportations?\s+de\s+([\d,]+)',
                    r'importations?\s+de\s+([\d,]+)',
                    r'balance\s+commerciale\s+([^.,;]+)',
                    r'sanctions?\s+√©conomiques\s+([^.,;]+)',
                    r'embargo\s+([^.,;]+)'
                ],
                'matches': []
            }
        }
        
        text_lower = text.lower()
        
        for category, data in indicators.items():
            for pattern in data['patterns']:
                matches = re.findall(pattern, text_lower, re.IGNORECASE)
                if matches:
                    data['matches'].extend(matches)
        
        # Nettoyer et formater les r√©sultats
        result = {}
        for category, data in indicators.items():
            if data['matches']:
                result[category] = list(set(data['matches']))[:5]  # Limiter √† 5 r√©sultats par cat√©gorie
        
        return result

    def identify_economic_sectors(self, text):
        """Identifie les secteurs √©conomiques concern√©s"""
        sectors = {
            '√©nergie': ['p√©trole', 'gaz', '√©lectricit√©', '√©nergie', 'renouvelable', 'nucl√©aire', 'OPEP'],
            'finance': ['banque', 'bourse', 'finance', 'investissement', 'cr√©dit', 'pr√™t', 'action'],
            'industrie': ['industrie', 'manufacturier', 'production', 'usine', 'automobile', 'a√©ronautique'],
            'technologie': ['technologie', 'digital', 'num√©rique', 'IA', 'intelligence artificielle', 'tech'],
            'agriculture': ['agriculture', 'agroalimentaire', 'cultures', 'r√©colte', 'ferme'],
            'transport': ['transport', 'logistique', 'a√©rien', 'maritime', 'routier'],
            'commerce': ['commerce', 'd√©tail', 'distribution', 'vente', 'magasin'],
            'tourisme': ['tourisme', 'h√¥tellerie', 'restauration', 'voyage']
        }
        
        detected_sectors = []
        text_lower = text.lower()
        
        for sector, keywords in sectors.items():
            if any(keyword in text_lower for keyword in keywords):
                detected_sectors.append(sector)
        
        return detected_sectors

    def assess_economic_impact(self, text):
        """√âvalue l'impact √©conomique potentiel"""
        impact_indicators = {
            'fort_positif': [
                'croissance record', 'hausse historique', 'rebond √©conomique', 
                'reprise vigoureuse', 'investissement massif', 'cr√©ation d\'emplois',
                'innovation majeure', 'accord commercial', 'partenariat strat√©gique'
            ],
            'positif': [
                'am√©lioration', 'progr√®s', 'augmentation', 'hausse', 'expansion',
                'd√©veloppement', 'investissement', 'croissance', 'emploi'
            ],
            'n√©gatif': [
                'r√©cession', 'crise √©conomique', 'chute', 'baisse', 'd√©clin',
                'ralentissement', 'contraction', 'licenciement', 'faillite'
            ],
            'fort_n√©gatif': [
                'effondrement', 'krach', 'd√©pression', 'catastrophe √©conomique',
                'effondrement boursier', 'crise financi√®re', 'faillite massive'
            ]
        }
        
        text_lower = text.lower()
        impact_score = 0
        
        for level, indicators in impact_indicators.items():
            weight = {
                'fort_positif': 2.0,
                'positif': 1.0,
                'n√©gatif': -1.0,
                'fort_n√©gatif': -2.0
            }[level]
            
            for indicator in indicators:
                if indicator in text_lower:
                    impact_score += weight
                    break  # Un indicateur par niveau suffit
        
        # Normaliser entre -1 et 1
        return max(-1, min(1, impact_score / 2))

    def detect_economic_trends(self, text):
        """D√©tecte les tendances √©conomiques mentionn√©es"""
        trends = {
            'hausse': [],
            'baisse': [],
            'stabilit√©': [],
            'volatilit√©': []
        }
        
        trend_patterns = {
            'hausse': [
                r'hausse\s+de\s+([\d,]+)%',
                r'augmentation\s+de\s+([\d,]+)%',
                r'croissance\s+de\s+([\d,]+)%',
                r'progresser?\s+de\s+([\d,]+)%'
            ],
            'baisse': [
                r'baisse\s+de\s+([\d,]+)%',
                r'chute\s+de\s+([\d,]+)%',
                r'd√©clin\s+de\s+([\d,]+)%',
                r'ralentissement\s+de\s+([\d,]+)%'
            ],
            'stabilit√©': [
                r'stable\s+√†\s+([\d,]+)',
                r'maintien\s+√†\s+([\d,]+)',
                r'stabilit√©\s+autour\s+de\s+([\d,]+)'
            ]
        }
        
        text_lower = text.lower()
        
        for trend, patterns in trend_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text_lower, re.IGNORECASE)
                if matches:
                    trends[trend].extend(matches)
        
        # D√©tection de volatilit√©
        volatility_indicators = [
            'volatilit√©', 'fluctuation', 'instabilit√©', 'incertitude', 'sp√©culation'
        ]
        if any(indicator in text_lower for indicator in volatility_indicators):
            trends['volatilit√©'].append('march√© volatile d√©tect√©')
        
        # Nettoyer les r√©sultats vides
        return {k: v for k, v in trends.items() if v}

    def generate_economic_recommendations(self, text):
        """G√©n√®re des recommandations bas√©es sur l'analyse √©conomique"""
        recommendations = []
        
        # Analyser l'impact √©conomique
        impact = self.assess_economic_impact(text)
        sectors = self.identify_economic_sectors(text)
        indicators = self.extract_economic_indicators(text)
        
        # Recommandations bas√©es sur l'impact
        if impact < -0.5:
            recommendations.append("üìâ IMPACT √âCONOMIQUE N√âGATIF - Surveillance des march√©s recommand√©e")
        elif impact > 0.5:
            recommendations.append("üìà IMPACT √âCONOMIQUE POSITIF - Opportunit√©s potentielles")
        
        # Recommandations bas√©es sur les secteurs
        if '√©nergie' in sectors:
            recommendations.append("‚ö° SECTEUR √âNERG√âTIQUE - Surveiller les prix des mati√®res premi√®res")
        
        if 'finance' in sectors:
            recommendations.append("üíπ SECTEUR FINANCIER - Analyser l'impact sur les march√©s")
        
        # Recommandations bas√©es sur les indicateurs
        if any('inflation' in str(indicator).lower() for category in indicators.values() for indicator in category):
            recommendations.append("üí∞ INFLATION D√âTECT√âE - Impact sur le pouvoir d'achat √† surveiller")
        
        if any('ch√¥mage' in str(indicator).lower() for category in indicators.values() for indicator in category):
            recommendations.append("üë• CH√îMAGE MENTIONN√â - Impact social et √©conomique √† analyser")
        
        # Recommandation par d√©faut si peu d'√©l√©ments d√©tect√©s
        if not recommendations and (sectors or indicators):
            recommendations.append("üìä ANALYSE √âCONOMIQUE - Contextualiser avec les donn√©es macro√©conomiques")
        
        return recommendations

    def analyze_geopolitical_context(self, article):
        """Analyse contextuelle g√©opolitique"""
        text = f"{article.get('title', '')} {article.get('content', '')}"
        
        return {
            'acteurs': self.extract_geopolitical_actors(text),
            'enjeux': self.extract_geopolitical_issues(text),
            'tensions': self.assess_geopolitical_tensions(text),
            'recommandations': self.generate_geopolitical_recommendations(text)
        }
    
    def extract_geopolitical_actors(self, text):
        """Extrait les acteurs g√©opolitiques"""
        actors = {
            'pays': re.findall(r'\b(France|Allemagne|√âtats-Unis|USA|China|Chine|Russie|UK|Royaume-Uni|Ukraine|Israel|Palestine)\b', text, re.IGNORECASE),
            'organisations': re.findall(r'\b(ONU|OTAN|UE|Union Europ√©enne|UN|NATO|OMS|WHO)\b', text, re.IGNORECASE),
            'dirigeants': re.findall(r'\b(Poutine|Zelensky|Macron|Biden|Xi|Merkel|Scholz)\b', text, re.IGNORECASE)
        }
        
        return {k: list(set(v)) for k, v in actors.items() if v}
    
    def extract_geopolitical_issues(self, text):
        """Extrait les enjeux g√©opolitiques"""
        issues = [
            'conflit territorial', 'sanctions √©conomiques', 'crise diplomatique',
            'accord commercial', 'coop√©ration militaire', 'tensions frontali√®res'
        ]
        
        detected_issues = []
        for issue in issues:
            if issue in text.lower():
                detected_issues.append(issue)
        
        return detected_issues
    
    def assess_geopolitical_tensions(self, text):
        """√âvalue les tensions g√©opolitiques"""
        tension_indicators = ['tension', 'conflit', 'crise', 'sanction', 'menace', 'hostilit√©']
        text_lower = text.lower()
        
        tension_score = sum(1 for indicator in tension_indicators if indicator in text_lower)
        return min(1.0, tension_score / 5)
    
    def generate_geopolitical_recommendations(self, text):
        """G√©n√®re des recommandations g√©opolitiques"""
        recommendations = []
        
        if self.assess_geopolitical_tensions(text) > 0.5:
            recommendations.append("‚ö†Ô∏è Tensions g√©opolitiques √©lev√©es - surveillance recommand√©e")
        
        actors = self.extract_geopolitical_actors(text)
        if len(actors.get('pays', [])) >= 3:
            recommendations.append("üåç Implication multiple de pays - analyse syst√©mique n√©cessaire")
        
        return recommendations
    
    def analyze_social_context(self, article):
        """Analyse contextuelle sociale"""
        return {
            'enjeux_sociaux': [],
            'mouvements_sociaux': [],
            'recommandations': ["Analyse sociale √† d√©velopper"]
        }
    
    def analyze_environmental_context(self, article):
        """Analyse contextuelle environnementale"""
        return {
            'enjeux_environnementaux': [],
            'impacts_climatiques': [],
            'recommandations': ["Analyse environnementale √† d√©velopper"]
        }
    
    def analyze_biases(self, article, contextual_analysis, web_research):
        """D√©tecte les biais potentiels"""
        biases = []
        text = f"{article.get('title', '')} {article.get('content', '')}"
        
        # Biais de langage
        if self.detect_emotional_language(text):
            biases.append("Langage √©motionnel d√©tect√©")
        
        # Biais de source
        if self.assess_source_credibility(article):
            biases.append("Source √† v√©rifier")
        
        # Biais de contexte
        if web_research and web_research.get('coherence', 1) < 0.7:
            biases.append("Divergence avec le contexte m√©diatique")
        
        return {
            'biais_d√©tect√©s': biases,
            'score_credibilite': self.calculate_credibility_score(biases, contextual_analysis),
            'recommandations': self.generate_bias_recommendations(biases)
        }
    
    def detect_emotional_language(self, text):
        """D√©tecte le langage √©motionnel"""
        emotional_words = [
            'incroyable', 'choquant', 'scandaleux', 'horrible', 'magnifique',
            'exceptionnel', 'catastrophique', 'dramatique'
        ]
        
        text_lower = text.lower()
        return any(word in text_lower for word in emotional_words)
    
    def assess_source_credibility(self, article):
        """√âvalue la cr√©dibilit√© de la source"""
        credible_sources = ['reuters', 'associated press', 'afp', 'bbc']
        source = article.get('feed', '').lower()
        
        return not any(credible in source for credible in credible_sources)
    
    def calculate_credibility_score(self, biases, contextual_analysis):
        """Calcule un score de cr√©dibilit√©"""
        base_score = 1.0
        
        # P√©nalit√©s pour les biais
        for bias in biases:
            if "Langage √©motionnel" in bias:
                base_score -= 0.2
            if "Source √† v√©rifier" in bias:
                base_score -= 0.3
            if "Divergence" in bias:
                base_score -= 0.2
        
        # Bonus pour l'urgence et l'impact (sujets importants)
        if contextual_analysis.get('urgence', 0) > 0.5:
            base_score += 0.1
        if contextual_analysis.get('impact', 0) > 0.5:
            base_score += 0.1
        
        return max(0, min(1, base_score))
    
    def generate_bias_recommendations(self, biases):
        """G√©n√®re des recommandations pour corriger les biais"""
        recommendations = []
        
        if "Langage √©motionnel" in str(biases):
            recommendations.append("Recadrer avec un langage plus neutre")
        
        if "Source √† v√©rifier" in str(biases):
            recommendations.append("Recouper avec des sources fiables")
        
        if "Divergence" in str(biases):
            recommendations.append("Contextualiser avec des informations v√©rifi√©es")
        
        return recommendations
    
    def synthesize_analysis(self, article, contextual_analysis, web_research, thematic_analysis, bias_analysis):
        """Synth√©tise toutes les analyses"""
        sentiment = article.get('sentiment', {})
        original_score = sentiment.get('score', 0)
        
        # Calcul du score corrig√© bas√© sur l'analyse approfondie
        corrected_score = self.calculate_corrected_score(
            original_score, 
            contextual_analysis, 
            web_research, 
            bias_analysis
        )
        
        return {
            'score_original': original_score,
            'score_corrig√©': corrected_score,
            'analyse_contextuelle': contextual_analysis,
            'recherche_web': web_research,
            'analyse_thematique': thematic_analysis,
            'analyse_biases': bias_analysis,
            'confiance': bias_analysis.get('score_credibilite', 0.5),
            'recommandations_globales': self.generate_global_recommendations(
                contextual_analysis, web_research, bias_analysis
            )
        }
    
    def calculate_corrected_score(self, original_score, contextual_analysis, web_research, bias_analysis):
        """Calcule le score corrig√© bas√© sur l'analyse approfondie"""
        correction = 0
        
        # Ajustement bas√© sur l'urgence
        urgency = contextual_analysis.get('urgence', 0)
        if urgency > 0.7:
            correction -= 0.1  # Les sujets urgents sont souvent plus n√©gatifs
        
        # Ajustement bas√© sur les tensions
        if 'g√©opolitique' in contextual_analysis:
            tensions = contextual_analysis.get('tensions', 0)
            if tensions > 0.5:
                correction -= 0.15
        
        # Ajustement bas√© sur la recherche web
        if web_research:
            web_sentiment = web_research.get('sentiment_moyen', 0)
            correction += (web_sentiment - original_score) * 0.3
        
        # Ajustement bas√© sur la cr√©dibilit√©
        credibility = bias_analysis.get('score_credibilite', 0.5)
        credibility_factor = credibility * 2 - 1  # Convertit 0-1 en -1 √† 1
        correction *= credibility_factor
        
        corrected = original_score + correction
        return max(-1, min(1, corrected))
    
    def generate_global_recommendations(self, contextual_analysis, web_research, bias_analysis):
        """G√©n√®re des recommandations globales"""
        recommendations = []
        
        # Recommandations bas√©es sur l'urgence
        if contextual_analysis.get('urgence', 0) > 0.7:
            recommendations.append("üö® SUJET URGENT - Surveillance renforc√©e recommand√©e")
        
        # Recommandations bas√©es sur la port√©e
        scope = contextual_analysis.get('port√©e', 'local')
        if scope == 'international':
            recommendations.append("üåç PORT√âE INTERNATIONALE - Analyse g√©opolitique approfondie")
        
        # Recommandations bas√©es sur la cr√©dibilit√©
        credibility = bias_analysis.get('score_credibilite', 0.5)
        if credibility < 0.7:
            recommendations.append("üîç CR√âDIBILIT√â √Ä V√âRIFIER - Recoupement des sources n√©cessaire")
        
        # Recommandations bas√©es sur la recherche web
        if web_research and web_research.get('coherence', 1) < 0.8:
            recommendations.append("üìä DIVERGENCE CONTEXTUELLE - Analyse comparative recommand√©e")
        
        return recommendations

# Initialiser l'analyseur IA avanc√©
advanced_analyzer = AdvancedIAAnalyzer()

@app.route('/correct_analysis', methods=['POST'])
def correct_analysis():
    """Endpoint pour corriger l'analyse des articles"""
    try:
        data = request.json or {}
        api_key = data.get('apiKey')
        articles = data.get('articles', [])
        current_analysis = data.get('currentAnalysis', {})
        themes = data.get('themes', [])
        
        if not api_key:
            return jsonify({'success': False, 'error': 'Cl√© API requise'})
        
        print(f"üß† Correction IA avanc√©e demand√©e pour {len(articles)} articles")
        
        corrections = []
        for article in articles[:10]:  # Limiter pour performance
            try:
                original_sentiment = article.get('sentiment', {})
                
                if original_sentiment:
                    # Analyse approfondie
                    deep_analysis = advanced_analyzer.perform_deep_analysis(article, themes)
                    
                    # CORRECTION : v√©rifier que les cl√©s existent
                    if deep_analysis and 'score_original' in deep_analysis and 'score_corrig√©' in deep_analysis:
                        corrections.append({
                            'articleId': article.get('id'),
                            'title': article.get('title'),
                            'originalScore': deep_analysis['score_original'],
                            'correctedScore': deep_analysis['score_corrig√©'],
                            'confidence': deep_analysis.get('confiance', 0.5),
                            'deepAnalysis': deep_analysis,
                            'timestamp': datetime.datetime.now().isoformat()
                        })
                    else:
                        print(f"‚ö†Ô∏è Analyse incompl√®te pour: {article.get('title', 'Unknown')[:50]}")
            except Exception as e:
                print(f"‚ùå Erreur analyse article {article.get('title', 'Unknown')[:50]}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"‚úÖ Corrections IA avanc√©es appliqu√©es: {len(corrections)} articles")
        
        return jsonify({
            'success': True,
            'corrections': corrections,
            'summary': {
                'total_corrected': len(corrections),
                'average_confidence': sum(c['confidence'] for c in corrections) / len(corrections) if corrections else 0,
                'deep_analysis_performed': len(corrections)
            }
        })
        
    except Exception as e:
        print(f"‚ùå Erreur correction IA avanc√©e: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

def generate_advanced_report(corrections, feed):
    """G√©n√®re un rapport PDF avanc√© avec analyse approfondie"""
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'rapport_analyse_avancee_{timestamp}.pdf'
    filepath = os.path.join(REPORTS_DIR, filename)

    # Cr√©ation du document PDF
    doc = SimpleDocTemplate(filepath, pagesize=A4)
    styles = getSampleStyleSheet()
    
    # Styles personnalis√©s
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Title'],
        fontSize=16,
        spaceAfter=30,
        textColor=colors.HexColor('#1e3a8a')
    )
    
    heading_style = ParagraphStyle(
        'CustomHeading',
        parent=styles['Heading2'],
        fontSize=12,
        spaceAfter=12,
        textColor=colors.HexColor('#374151')
    )
    
    elements = []

    # En-t√™te du rapport
    elements.append(Paragraph("RAPPORT D'ANALYSE AVANC√âE - GEOPOLIS IA", title_style))
    elements.append(Paragraph(f"G√©n√©r√© le: {datetime.datetime.now().strftime('%d/%m/%Y √† %H:%M')}", styles['Normal']))
    elements.append(Spacer(1, 20))

    # R√©sum√© ex√©cutif
    elements.append(Paragraph("üìä R√âSUM√â EX√âCUTIF", heading_style))
    
    executive_summary = generate_executive_summary(corrections)
    for paragraph in executive_summary:
        elements.append(Paragraph(paragraph, styles['Normal']))
    
    elements.append(Spacer(1, 15))

    # Analyse par article
    elements.append(Paragraph("üîç ANALYSE D√âTAILL√âE PAR ARTICLE", heading_style))
    
    for i, correction in enumerate(corrections[:5], 1):  # Limiter √† 5 articles
        elements.append(Paragraph(f"Article {i}: {correction.get('title', 'Sans titre')[:80]}...", styles['Heading3']))
        
        # Score et correction
        original_score = correction.get('originalScore', 0)
        corrected_score = correction.get('correctedScore', 0)
        score_diff = corrected_score - original_score
        
        score_text = f"Score: {original_score:.2f} ‚Üí {corrected_score:.2f} (Œî{score_diff:+.2f})"
        elements.append(Paragraph(score_text, styles['Normal']))
        
        # Analyse contextuelle
        deep_analysis = correction.get('deepAnalysis', {})
        context = deep_analysis.get('analyse_contextuelle', {})
        
        if context:
            elements.append(Paragraph("üìà Analyse Contextuelle:", styles['Heading4']))
            elements.append(Paragraph(f"‚Ä¢ Urgence: {context.get('urgence', 0):.1%}", styles['Normal']))
            elements.append(Paragraph(f"‚Ä¢ Port√©e: {context.get('port√©e', 'N/A')}", styles['Normal']))
            elements.append(Paragraph(f"‚Ä¢ Impact: {context.get('impact', 0):.1%}", styles['Normal']))
        
        # Recommandations
        recommendations = deep_analysis.get('recommandations_globales', [])
        if recommendations:
            elements.append(Paragraph("üí° Recommandations:", styles['Heading4']))
            for rec in recommendations[:3]:
                elements.append(Paragraph(f"‚Ä¢ {rec}", styles['Normal']))
        
        elements.append(Spacer(1, 10))

    # Tendances et insights
    elements.append(Paragraph("üìà TENDANCES ET INSIGHTS", heading_style))
    trends_analysis = analyze_trends(corrections)
    for trend in trends_analysis:
        elements.append(Paragraph(f"‚Ä¢ {trend}", styles['Normal']))
    
    elements.append(Spacer(1, 15))

    # M√©thodologie
    elements.append(Paragraph("üîß M√âTHODOLOGIE", heading_style))
    methodology = [
        "‚Ä¢ Analyse contextuelle multi-niveaux",
        "‚Ä¢ Recherche web sur sources fiables", 
        "‚Ä¢ D√©tection de biais et cr√©dibilit√©",
        "‚Ä¢ Analyse g√©opolitique sp√©cialis√©e",
        "‚Ä¢ Synth√®se par intelligence artificielle"
    ]
    
    for item in methodology:
        elements.append(Paragraph(item, styles['Normal']))

    # Construction du PDF
    doc.build(elements)
    print(f"üìä Rapport avanc√© g√©n√©r√©: {filename}")
    
    return filename

def generate_executive_summary(corrections):
    """G√©n√®re un r√©sum√© ex√©cutif intelligent"""
    if not corrections:
        return ["Aucune analyse disponible."]
    
    total_articles = len(corrections)
    significant_corrections = [c for c in corrections if abs(c.get('correctedScore', 0) - c.get('originalScore', 0)) > 0.2]
    
    high_impact = [c for c in corrections 
                   if c.get('deepAnalysis', {}).get('analyse_contextuelle', {}).get('impact', 0) > 0.7]
    urgent_articles = [c for c in corrections 
                      if c.get('deepAnalysis', {}).get('analyse_contextuelle', {}).get('urgence', 0) > 0.7]
    
    summary = [
        f"üìä Analyse de {total_articles} articles avec intelligence artificielle avanc√©e.",
        f"üîß {len(significant_corrections)} corrections significatives appliqu√©es.",
        f"üö® {len(urgent_articles)} sujets identifi√©s comme urgents n√©cessitant une attention particuli√®re.",
        f"üí• {len(high_impact)} articles √† fort impact d√©tect√©s.",
        "",
        "üîç PRINCIPAUX ENSEIGNEMENTS:"
    ]
    
    # Ajouter des insights sp√©cifiques
    if urgent_articles:
        summary.append("‚Ä¢ Pr√©sence de sujets urgents n√©cessitant un suivi rapproch√©")
    
    if len(significant_corrections) > total_articles * 0.3:
        summary.append("‚Ä¢ Correction importante des scores initiaux - am√©lioration de la pr√©cision")
    
    avg_confidence = sum(c.get('confidence', 0) for c in corrections) / len(corrections) if corrections else 0
    if avg_confidence > 0.8:
        summary.append("‚Ä¢ Forte confiance dans les analyses r√©alis√©es")
    elif avg_confidence < 0.6:
        summary.append("‚Ä¢ Attention: confiance mod√©r√©e dans certaines analyses")
    
    return summary

def analyze_trends(corrections):
    """Analyse les tendances globales"""
    if not corrections:
        return ["Aucune donn√©e pour l'analyse des tendances."]
    
    trends = []
    
    # Tendances des corrections
    corrections_values = [abs(c.get('correctedScore', 0) - c.get('originalScore', 0)) for c in corrections]
    avg_correction = sum(corrections_values) / len(corrections_values) if corrections_values else 0
    
    if avg_correction > 0.3:
        trends.append("Corrections importantes: scores initiaux significativement ajust√©s")
    elif avg_correction > 0.1:
        trends.append("Corrections mod√©r√©es: ajustements contextuels appliqu√©s")
    else:
        trends.append("Coh√©rence g√©n√©rale: scores initiaux globalement fiables")
    
    # Tendances d'urgence
    urgent_count = sum(1 for c in corrections 
                      if c.get('deepAnalysis', {}).get('analyse_contextuelle', {}).get('urgence', 0) > 0.5)
    if urgent_count > len(corrections) * 0.3:
        trends.append("Contexte tendu: proportion √©lev√©e de sujets urgents")
    
    # Tendances de cr√©dibilit√©
    credibility_scores = [c.get('deepAnalysis', {}).get('analyse_biases', {}).get('score_credibilite', 0.5) 
                         for c in corrections]
    avg_credibility = sum(credibility_scores) / len(credibility_scores) if credibility_scores else 0.5
    
    if avg_credibility < 0.6:
        trends.append("Vigilance cr√©dibilit√©: certaines sources n√©cessitent v√©rification")
    
    return trends

@app.route('/analyze_full', methods=['POST'])
def analyze_full():
    """Analyse compl√®te avec g√©n√©ration de rapport avanc√©"""
    try:
        data = request.json or {}
        feed = data.get('feed', {})
        api_key = data.get('apiKey')

        if not api_key:
            return jsonify({'success': False, 'error': 'Cl√© API requise'})

        print("üß† D√©but de l'analyse IA avanc√©e...")
        
        # Analyse approfondie
        articles = feed.get('articles', [])
        themes = feed.get('themes', [])
        corrections = []
        
        for article in articles[:8]:  # Limiter pour performance
            try:
                original_sentiment = article.get('sentiment', {})
                if original_sentiment:
                    deep_analysis = advanced_analyzer.perform_deep_analysis(article, themes)
                    
                    if deep_analysis and 'score_original' in deep_analysis and 'score_corrig√©' in deep_analysis:
                        corrections.append({
                            'title': article.get('title'),
                            'originalScore': deep_analysis['score_original'],
                            'correctedScore': deep_analysis['score_corrig√©'],
                            'confidence': deep_analysis.get('confiance', 0.5),
                            'deepAnalysis': deep_analysis
                        })
            except Exception as e:
                print(f"‚ùå Erreur analyse article: {e}")
                continue

        # G√©n√©ration du rapport avanc√©
        report_filename = generate_advanced_report(corrections, feed)
        report_url = f'http://localhost:5051/reports/{report_filename}'
        
        print("‚úÖ Analyse IA avanc√©e termin√©e")
        
        return jsonify({
            'success': True,
            'report_url': report_url,
            'corrections_applied': len(corrections),
            'average_confidence': sum(c['confidence'] for c in corrections) / len(corrections) if corrections else 0,
            'analysis_metadata': {
                'articles_analyzed': len(corrections),
                'deep_analysis_performed': True,
                'contextual_research': True,
                'bias_detection': True,
                'generated_at': datetime.datetime.now().isoformat()
            }
        })

    except Exception as e:
        print(f"‚ùå Erreur analyse avanc√©e: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

# Routes de service
@app.route('/reports/<path:filename>', methods=['GET'])
def serve_report(filename):
    return send_from_directory(REPORTS_DIR, filename, as_attachment=False)

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'service': 'Advanced IA Analysis Service',
        'features': ['deep_context_analysis', 'web_research', 'bias_detection', 'advanced_reporting'],
        'timestamp': datetime.datetime.now().isoformat()
    })

if __name__ == '__main__':
    print("üöÄ Service IA Avanc√© d√©marr√© sur http://localhost:5051")
    print("üß† Fonctionnalit√©s activ√©es:")
    print("   - Analyse contextuelle avanc√©e")
    print("   - Recherche web contextuelle") 
    print("   - D√©tection de biais et cr√©dibilit√©")
    print("   - Analyse g√©opolitique sp√©cialis√©e")
    print("   - Rapports PDF intelligents")
    print("   - Synth√®se par IA avanc√©e")
    
    app.run(host='0.0.0.0', port=5051, debug=True)