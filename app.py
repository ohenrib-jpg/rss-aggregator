import os
import json
import datetime
import requests
from flask import Flask, request, jsonify, send_from_directory
from bs4 import BeautifulSoup
import re
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.platypus.flowables import Spacer
import matplotlib.pyplot as plt
import io
import base64
from analysis_utils import ensure_deep_analysis_consistency, compute_confidence_from_features, clamp01
from modules.storage_manager import save_analysis_batch, load_recent_analyses
from modules.corroboration import find_corroborations

app = Flask(__name__)
REPORTS_DIR = os.path.join(os.path.dirname(__file__), 'reports')
os.makedirs(REPORTS_DIR, exist_ok=True)

# Service de recherche web avanc√©
class AdvancedWebResearch:
    def __init__(self):
        self.trusted_sources = [
            'reuters.com', 'apnews.com', 'bbc.com', 'theguardian.com',
            'lemonde.fr', 'liberation.fr', 'figaro.fr', 'france24.com'
        ]
    
    def search_contextual_info(self, article_title, themes):
        """Recherche des informations contextuelles sur le web"""
        try:
            # Recherche sur des sources fiables
            search_terms = self.build_search_query(article_title, themes)
            contextual_data = []
            
            for source in self.trusted_sources[:2]:  # Limiter pour performance
                try:
                    data = self.search_on_source(source, search_terms)
                    if data:
                        contextual_data.append(data)
                except Exception as e:
                    print(f"‚ùå Erreur recherche {source}: {e}")
            
            # CORRECTION : utiliser article_title au lieu de original_title
            return self.analyze_contextual_data(contextual_data, article_title)
            
        except Exception as e:
            print(f"‚ùå Erreur recherche contextuelle: {e}")
            return None
    
    def build_search_query(self, title, themes):
        """Construit une requ√™te de recherche optimis√©e"""
        # Extraire les entit√©s nomm√©es
        entities = self.extract_entities(title)
        
        # CORRECTION : g√©rer les th√®mes comme liste de strings
        theme_keywords = ''
        if themes:
            if isinstance(themes, list):
                theme_keywords = ' OR '.join(str(t) for t in themes[:3])
            else:
                theme_keywords = str(themes)
        
        query = f"({title}) {theme_keywords}"
        if entities:
            query += f" {' '.join(entities)}"
        
        return query
    
    def extract_entities(self, text):
        """Extraction basique d'entit√©s nomm√©es"""
        # Patterns pour les entit√©s g√©opolitiques
        patterns = {
            'pays': r'\b(France|Allemagne|√âtats-Unis|USA|China|Chine|Russie|UK|Royaume-Uni|Ukraine|Israel|Palestine)\b',
            'organisations': r'\b(ONU|OTAN|UE|Union Europ√©enne|UN|NATO|OMS|WHO)\b',
            'personnes': r'\b(Poutine|Zelensky|Macron|Biden|Xi|Merkel|Scholz)\b'
        }
        
        entities = []
        for category, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            entities.extend(matches)
        
        return entities
    
    def search_on_source(self, source, query):
        """Recherche sur une source sp√©cifique (simul√©e pour l'instant)"""
        # Impl√©mentation simul√©e - √† remplacer par une vraie recherche
        return {
            'source': source,
            'title': f"Article contextuel sur {query}",
            'content': f"Informations contextuelles r√©cup√©r√©es de {source} concernant {query}",
            'sentiment': 'neutral',
            'date': datetime.datetime.now().isoformat()
        }
    
    def analyze_contextual_data(self, contextual_data, article_title):
        """Analyse les donn√©es contextuelles pour d√©tecter les divergences"""
        if not contextual_data:
            return None
        
        # Analyse de coh√©rence
        sentiment_scores = []
        key_facts = []
        
        for data in contextual_data:
            sentiment = self.analyze_sentiment(data['content'])
            sentiment_scores.append(sentiment)
            key_facts.extend(self.extract_key_facts(data['content']))
        
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
        
        return {
            'sources_consult√©es': len(contextual_data),
            'sentiment_moyen': avg_sentiment,
            'faits_cles': list(set(key_facts))[:5],  # D√©dupliquer et limiter
            'coherence': self.calculate_coherence(sentiment_scores),
            'recommendations': self.generate_recommendations(avg_sentiment, key_facts)
        }
    
    def analyze_sentiment(self, text):
        """Analyse de sentiment simplifi√©e"""
        positive_words = ['accord', 'paix', 'progr√®s', 'succ√®s', 'coop√©ration', 'dialogue']
        negative_words = ['conflit', 'crise', 'tension', 'sanction', 'violence', 'protestation']
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        total = positive_count + negative_count
        if total == 0:
            return 0
        
        return (positive_count - negative_count) / total
    
    def extract_key_facts(self, text):
        """Extraction de faits cl√©s"""
        facts = []
        
        # Patterns pour les faits importants
        fact_patterns = [
            r'accord sur\s+([^.,]+)',
            r'sanctions?\s+contre\s+([^.,]+)',
            r'crise\s+(?:au|en)\s+([^.,]+)',
            r'n√©gociations?\s+(?:√†|en)\s+([^.,]+)'
        ]
        
        for pattern in fact_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            facts.extend(matches)
        
        return facts
    
    def calculate_coherence(self, sentiment_scores):
        """Calcule la coh√©rence entre les sources"""
        if len(sentiment_scores) < 2:
            return 1.0
        
        variance = sum((score - sum(sentiment_scores)/len(sentiment_scores))**2 for score in sentiment_scores)
        return max(0, 1 - variance)
    
    def generate_recommendations(self, sentiment, key_facts):
        """G√©n√®re des recommandations bas√©es sur l'analyse"""
        recommendations = []
        
        if abs(sentiment) > 0.3:
            recommendations.append("√âcart sentiment d√©tect√© - v√©rification recommand√©e")
        
        if key_facts:
            recommendations.append(f"Faits contextuels identifi√©s: {', '.join(key_facts[:3])}")
        
        if len(recommendations) == 0:
            recommendations.append("Coh√©rence g√©n√©rale avec le contexte m√©diatique")
        
        return recommendations

# Analyseur IA avanc√© avec raisonnement
class AdvancedIAAnalyzer:
    def __init__(self):
        self.web_research = AdvancedWebResearch()
        self.analysis_framework = {
            'g√©opolitique': self.analyze_geopolitical_context,
            '√©conomique': self.analyze_economic_context,
            'social': self.analyze_social_context,
            'environnement': self.analyze_environmental_context
        }
    
    def perform_deep_analysis(self, article, themes):
        """Analyse approfondie avec raisonnement"""
        print(f"üß† Analyse approfondie: {article.get('title', '')[:50]}...")
        
        try:
            # CORRECTION : extraire les noms des th√®mes
            theme_names = []
            if themes:
                if isinstance(themes, list):
                    theme_names = [t.get('name', str(t)) if isinstance(t, dict) else str(t) for t in themes]
                else:
                    theme_names = [str(themes)]
            
            # 1. Analyse contextuelle avanc√©e
            contextual_analysis = self.analyze_advanced_context(article, theme_names)
            
            # 2. Recherche web pour v√©rification
            web_research = self.web_research.search_contextual_info(
                article.get('title', ''), 
                theme_names
            )
            
            # 3. Analyse th√©matique sp√©cialis√©e
            thematic_analysis = self.analyze_thematic_context(article, theme_names)
            
            # 4. D√©tection de biais et v√©rification
            bias_analysis = self.analyze_biases(article, contextual_analysis, web_research)
            
            # 5. Synth√®se et recommandations
            final_analysis = self.synthesize_analysis(
                article, 
                contextual_analysis, 
                web_research, 
                thematic_analysis, 
                bias_analysis
            )
            
            return final_analysis
            
        except Exception as e:
            print(f"‚ùå Erreur analyse approfondie: {e}")
            import traceback
            traceback.print_exc()
            
            # Retourner une analyse par d√©faut en cas d'erreur
            sentiment = article.get('sentiment', {})
            return {
                'score_original': sentiment.get('score', 0),
                'score_corrected': sentiment.get('score', 0),
                'confidence': 0.3,
                'analyse_contextuelle': {},
                'recherche_web': None,
                'analyse_thematique': {},
                'analyse_biases': {'biais_d√©tect√©s': [], 'score_credibilite': 0.5},
                'recommandations_globales': ['Erreur lors de l\'analyse approfondie']
            }
    
    def analyze_advanced_context(self, article, themes):
        """Analyse contextuelle avanc√©e"""
        title = article.get('title', '')
        content = article.get('content', '')
        full_text = f"{title} {content}"
        
        analysis = {
            'urgence': self.assess_urgency(full_text),
            'port√©e': self.assess_scope(full_text),
            'impact': self.assess_impact(full_text, themes),
            'nouveaut√©': self.assess_novelty(full_text),
            'controverses': self.detect_controversies(full_text)
        }
        
        return analysis
    
    def assess_urgency(self, text):
        """√âvalue l'urgence de l'information"""
        urgent_indicators = ['urgence', 'crise', 'imm√©diat', 'drame', 'catastrophe', 'attaque']
        text_lower = text.lower()
        
        urgency_score = sum(1 for indicator in urgent_indicators if indicator in text_lower)
        return min(1.0, urgency_score / 3)
    
    def assess_scope(self, text):
        """√âvalue la port√©e g√©ographique"""
        scopes = {
            'local': ['ville', 'r√©gion', 'local', 'municipal'],
            'national': ['France', 'pays', 'national', 'gouvernement'],
            'international': ['monde', 'international', 'ONU', 'OTAN', 'UE']
        }
        
        text_lower = text.lower()
        scope_scores = {}
        
        for scope, indicators in scopes.items():
            score = sum(1 for indicator in indicators if indicator in text_lower)
            scope_scores[scope] = score
        
        return max(scope_scores, key=scope_scores.get) if scope_scores else 'local'
    
    def assess_impact(self, text, themes):
        """√âvalue l'impact potentiel"""
        high_impact_indicators = [
            'crise', 'r√©cession', 'guerre', 'sanctions', 'accord historique',
            'rupture', 'r√©volution', 'transition'
        ]
        
        text_lower = text.lower()
        impact_score = sum(1 for indicator in high_impact_indicators if indicator in text_lower)
        
        # Pond√©ration par th√®me
        theme_weights = {
            'conflit': 1.5, '√©conomie': 1.3, 'diplomatie': 1.2,
            'environnement': 1.1, 'social': 1.0
        }
        
        theme_weight = 1.0
        for theme in themes:
            theme_lower = str(theme).lower() if theme else ''
            if theme_lower in theme_weights:
                theme_weight = max(theme_weight, theme_weights[theme_lower])
        
        return min(1.0, (impact_score / 5) * theme_weight)
    
    def assess_novelty(self, text):
        """√âvalue la nouveaut√© de l'information"""
        novel_indicators = [
            'nouveau', 'premier', 'historique', 'inaugural', 'innovation',
            'r√©volutionnaire', 'changement', 'r√©forme'
        ]
        
        text_lower = text.lower()
        novelty_score = sum(1 for indicator in novel_indicators if indicator in text_lower)
        return min(1.0, novelty_score / 4)
    
    def detect_controversies(self, text):
        """D√©tecte les controverses potentielles"""
        controversy_indicators = [
            'pol√©mique', 'controvers√©', 'd√©bat', 'opposition', 'critique',
            'protestation', 'manifestation', 'conflit d\'int√©r√™t'
        ]
        
        text_lower = text.lower()
        controversies = []
        
        for indicator in controversy_indicators:
            if indicator in text_lower:
                # Trouver le contexte autour de l'indicateur
                start = max(0, text_lower.find(indicator) - 50)
                end = min(len(text), text_lower.find(indicator) + len(indicator) + 50)
                context = text[start:end].strip()
                controversies.append(f"{indicator}: {context}")
        
        return controversies
    
    def analyze_thematic_context(self, article, themes):
        """Analyse contextuelle par th√®me"""
        thematic_analysis = {}
        
        # CORRECTION : s'assurer que themes est une liste de cha√Ænes
        theme_list = []
        if themes:
            if isinstance(themes, list):
                theme_list = [str(t) for t in themes]
            else:
                theme_list = [str(themes)]
        
        for theme in theme_list:
            theme_lower = theme.lower() if theme else ''
            
            if theme_lower in self.analysis_framework:
                try:
                    analysis = self.analysis_framework[theme_lower](article)
                    thematic_analysis[theme] = analysis
                except Exception as e:
                    print(f"‚ùå Erreur analyse th√®me {theme}: {e}")
        
        return thematic_analysis

    def analyze_economic_context(self, article):
        """Analyse contextuelle √©conomique"""
        text = f"{article.get('title', '')} {article.get('content', '')}"
        
        return {
            'indicateurs': self.extract_economic_indicators(text),
            'secteurs': self.identify_economic_sectors(text),
            'impact_economique': self.assess_economic_impact(text),
            'tendances': self.detect_economic_trends(text),
            'recommandations': self.generate_economic_recommendations(text)
        }

    def extract_economic_indicators(self, text):
        """Extrait les indicateurs √©conomiques mentionn√©s"""
        indicators = {
            'macro√©conomiques': {
                'patterns': [
                    r'PIB\s*(?:de|du|\s)([^.,;]+)',
                    r'croissance\s+√©conomique\s+de\s+([\d,]+)%',
                    r'inflation\s+de\s+([\d,]+)%',
                    r'ch√¥mage\s+de\s+([\d,]+)%',
                    r'dette\s+publique\s+de\s+([\d,]+)',
                    r'd√©ficit\s+budg√©taire\s+de\s+([\d,]+)'
                ],
                'matches': []
            },
            'financiers': {
                'patterns': [
                    r'march√©s?\s+boursiers?\s+([^.,;]+)',
                    r'indice\s+([A-Z]+)\s+([\d,]+)',
                    r'euro\s+([\d,]+)\s+dollars?',
                    r'dollar\s+([\d,]+)\s+euros?',
                    r'taux\s+directeur\s+([^.,;]+)',
                    r'banque\s+centrale\s+([^.,;]+)'
                ],
                'matches': []
            },
            'commerciaux': {
                'patterns': [
                    r'commerce\s+ext√©rieur\s+([^.,;]+)',
                    r'exportations?\s+de\s+([\d,]+)',
                    r'importations?\s+de\s+([\d,]+)',
                    r'balance\s+commerciale\s+([^.,;]+)',
                    r'sanctions?\s+√©conomiques\s+([^.,;]+)',
                    r'embargo\s+([^.,;]+)'
                ],
                'matches': []
            }
        }
        
        text_lower = text.lower()
        
        for category, data in indicators.items():
            for pattern in data['patterns']:
                matches = re.findall(pattern, text_lower, re.IGNORECASE)
                if matches:
                    data['matches'].extend(matches)
        
        # Nettoyer et formater les r√©sultats
        result = {}
        for category, data in indicators.items():
            if data['matches']:
                result[category] = list(set(data['matches']))[:5]  # Limiter √† 5 r√©sultats par cat√©gorie
        
        return result

    def identify_economic_sectors(self, text):
        """Identifie les secteurs √©conomiques concern√©s"""
        sectors = {
            '√©nergie': ['p√©trole', 'gaz', '√©lectricit√©', '√©nergie', 'renouvelable', 'nucl√©aire', 'OPEP'],
            'finance': ['banque', 'bourse', 'finance', 'investissement', 'cr√©dit', 'pr√™t', 'action'],
            'industrie': ['industrie', 'manufacturier', 'production', 'usine', 'automobile', 'a√©ronautique'],
            'technologie': ['technologie', 'digital', 'num√©rique', 'IA', 'intelligence artificielle', 'tech'],
            'agriculture': ['agriculture', 'agroalimentaire', 'cultures', 'r√©colte', 'ferme'],
            'transport': ['transport', 'logistique', 'a√©rien', 'maritime', 'routier'],
            'commerce': ['commerce', 'd√©tail', 'distribution', 'vente', 'magasin'],
            'tourisme': ['tourisme', 'h√¥tellerie', 'restauration', 'voyage']
        }
        
        detected_sectors = []
        text_lower = text.lower()
        
        for sector, keywords in sectors.items():
            if any(keyword in text_lower for keyword in keywords):
                detected_sectors.append(sector)
        
        return detected_sectors

    def assess_economic_impact(self, text):
        """√âvalue l'impact √©conomique potentiel"""
        impact_indicators = {
            'fort_positif': [
                'croissance record', 'hausse historique', 'rebond √©conomique', 
                'reprise vigoureuse', 'investissement massif', 'cr√©ation d\'emplois',
                'innovation majeure', 'accord commercial', 'partenariat strat√©gique'
            ],
            'positif': [
                'am√©lioration', 'progr√®s', 'augmentation', 'hausse', 'expansion',
                'd√©veloppement', 'investissement', 'croissance', 'emploi'
            ],
            'n√©gatif': [
                'r√©cession', 'crise √©conomique', 'chute', 'baisse', 'd√©clin',
                'ralentissement', 'contraction', 'licenciement', 'faillite'
            ],
            'fort_n√©gatif': [
                'effondrement', 'krach', 'd√©pression', 'catastrophe √©conomique',
                'effondrement boursier', 'crise financi√®re', 'faillite massive'
            ]
        }
        
        text_lower = text.lower()
        impact_score = 0
        
        for level, indicators in impact_indicators.items():
            weight = {
                'fort_positif': 2.0,
                'positif': 1.0,
                'n√©gatif': -1.0,
                'fort_n√©gatif': -2.0
            }[level]
            
            for indicator in indicators:
                if indicator in text_lower:
                    impact_score += weight
                    break  # Un indicateur par niveau suffit
        
        # Normaliser entre -1 et 1
        return max(-1, min(1, impact_score / 2))

    def detect_economic_trends(self, text):
        """D√©tecte les tendances √©conomiques mentionn√©es"""
        trends = {
            'hausse': [],
            'baisse': [],
            'stabilit√©': [],
            'volatilit√©': []
        }
        
        trend_patterns = {
            'hausse': [
                r'hausse\s+de\s+([\d,]+)%',
                r'augmentation\s+de\s+([\d,]+)%',
                r'croissance\s+de\s+([\d,]+)%',
                r'progresser?\s+de\s+([\d,]+)%'
            ],
            'baisse': [
                r'baisse\s+de\s+([\d,]+)%',
                r'chute\s+de\s+([\d,]+)%',
                r'd√©clin\s+de\s+([\d,]+)%',
                r'ralentissement\s+de\s+([\d,]+)%'
            ],
            'stabilit√©': [
                r'stable\s+√†\s+([\d,]+)',
                r'maintien\s+√†\s+([\d,]+)',
                r'stabilit√©\s+autour\s+de\s+([\d,]+)'
            ]
        }
        
        text_lower = text.lower()
        
        for trend, patterns in trend_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text_lower, re.IGNORECASE)
                if matches:
                    trends[trend].extend(matches)
        
        # D√©tection de volatilit√©
        volatility_indicators = [
            'volatilit√©', 'fluctuation', 'instabilit√©', 'incertitude', 'sp√©culation'
        ]
        if any(indicator in text_lower for indicator in volatility_indicators):
            trends['volatilit√©'].append('march√© volatile d√©tect√©')
        
        # Nettoyer les r√©sultats vides
        return {k: v for k, v in trends.items() if v}

    def generate_economic_recommendations(self, text):
        """G√©n√®re des recommandations bas√©es sur l'analyse √©conomique"""
        recommendations = []
        
        # Analyser l'impact √©conomique
        impact = self.assess_economic_impact(text)
        sectors = self.identify_economic_sectors(text)
        indicators = self.extract_economic_indicators(text)
        
        # Recommandations bas√©es sur l'impact
        if impact < -0.5:
            recommendations.append("üìâ IMPACT √âCONOMIQUE N√âGATIF - Surveillance des march√©s recommand√©e")
        elif impact > 0.5:
            recommendations.append("üìà IMPACT √âCONOMIQUE POSITIF - Opportunit√©s potentielles")
        
        # Recommandations bas√©es sur les secteurs
        if '√©nergie' in sectors:
            recommendations.append("‚ö° SECTEUR √âNERG√âTIQUE - Surveiller les prix des mati√®res premi√®res")
        
        if 'finance' in sectors:
            recommendations.append("üíπ SECTEUR FINANCIER - Analyser l'impact sur les march√©s")
        
        # Recommandations bas√©es sur les indicateurs
        if any('inflation' in str(indicator).lower() for category in indicators.values() for indicator in category):
            recommendations.append("üí∞ INFLATION D√âTECT√âE - Impact sur le pouvoir d'achat √† surveiller")
        
        if any('ch√¥mage' in str(indicator).lower() for category in indicators.values() for indicator in category):
            recommendations.append("üë• CH√îMAGE MENTIONN√â - Impact social et √©conomique √† analyser")
        
        # Recommandation par d√©faut si peu d'√©l√©ments d√©tect√©s
        if not recommendations and (sectors or indicators):
            recommendations.append("üìä ANALYSE √âCONOMIQUE - Contextualiser avec les donn√©es macro√©conomiques")
        
        return recommendations

    def analyze_geopolitical_context(self, article):
        """Analyse contextuelle g√©opolitique"""
        text = f"{article.get('title', '')} {article.get('content', '')}"
        
        return {
            'acteurs': self.extract_geopolitical_actors(text),
            'enjeux': self.extract_geopolitical_issues(text),
            'tensions': self.assess_geopolitical_tensions(text),
            'recommandations': self.generate_geopolitical_recommendations(text)
        }
    
    def extract_geopolitical_actors(self, text):
        """Extrait les acteurs g√©opolitiques"""
        actors = {
            'pays': re.findall(r'\b(France|Allemagne|√âtats-Unis|USA|China|Chine|Russie|UK|Royaume-Uni|Ukraine|Israel|Palestine)\b', text, re.IGNORECASE),
            'organisations': re.findall(r'\b(ONU|OTAN|UE|Union Europ√©enne|UN|NATO|OMS|WHO)\b', text, re.IGNORECASE),
            'dirigeants': re.findall(r'\b(Poutine|Zelensky|Macron|Biden|Xi|Merkel|Scholz)\b', text, re.IGNORECASE)
        }
        
        return {k: list(set(v)) for k, v in actors.items() if v}
    
    def extract_geopolitical_issues(self, text):
        """Extrait les enjeux g√©opolitiques"""
        issues = [
            'conflit territorial', 'sanctions √©conomiques', 'crise diplomatique',
            'accord commercial', 'coop√©ration militaire', 'tensions frontali√®res'
        ]
        
        detected_issues = []
        for issue in issues:
            if issue in text.lower():
                detected_issues.append(issue)
        
        return detected_issues
    
    def assess_geopolitical_tensions(self, text):
        """√âvalue les tensions g√©opolitiques"""
        tension_indicators = ['tension', 'conflit', 'crise', 'sanction', 'menace', 'hostilit√©']
        text_lower = text.lower()
        
        tension_score = sum(1 for indicator in tension_indicators if indicator in text_lower)
        return min(1.0, tension_score / 5)
    
    def generate_geopolitical_recommendations(self, text):
        """G√©n√®re des recommandations g√©opolitiques"""
        recommendations = []
        
        if self.assess_geopolitical_tensions(text) > 0.5:
            recommendations.append("‚ö†Ô∏è Tensions g√©opolitiques √©lev√©es - surveillance recommand√©e")
        
        actors = self.extract_geopolitical_actors(text)
        if len(actors.get('pays', [])) >= 3:
            recommendations.append("üåç Implication multiple de pays - analyse syst√©mique n√©cessaire")
        
        return recommendations
    
    def analyze_social_context(self, article):
        """Analyse contextuelle sociale"""
        return {
            'enjeux_sociaux': [],
            'mouvements_sociaux': [],
            'recommandations': ["Analyse sociale √† d√©velopper"]
        }
    
    def analyze_environmental_context(self, article):
        """Analyse contextuelle environnementale"""
        return {
            'enjeux_environnementaux': [],
            'impacts_climatiques': [],
            'recommandations': ["Analyse environnementale √† d√©velopper"]
        }
    
    def analyze_biases(self, article, contextual_analysis, web_research):
        """D√©tecte les biais potentiels"""
        biases = []
        text = f"{article.get('title', '')} {article.get('content', '')}"
        
        # Biais de langage
        if self.detect_emotional_language(text):
            biases.append("Langage √©motionnel d√©tect√©")
        
        # Biais de source
        if self.assess_source_credibility(article):
            biases.append("Source √† v√©rifier")
        
        # Biais de contexte
        if web_research and web_research.get('coherence', 1) < 0.7:
            biases.append("Divergence avec le contexte m√©diatique")
        
        return {
            'biais_d√©tect√©s': biases,
            'score_credibilite': self.calculate_credibility_score(biases, contextual_analysis),
            'recommandations': self.generate_bias_recommendations(biases)
        }
    
    def detect_emotional_language(self, text):
        """D√©tecte le langage √©motionnel"""
        emotional_words = [
            'incroyable', 'choquant', 'scandaleux', 'horrible', 'magnifique',
            'exceptionnel', 'catastrophique', 'dramatique'
        ]
        
        text_lower = text.lower()
        return any(word in text_lower for word in emotional_words)
    
    def assess_source_credibility(self, article):
        """√âvalue la cr√©dibilit√© de la source"""
        credible_sources = ['reuters', 'associated press', 'afp', 'bbc']
        source = article.get('feed', '').lower()
        
        return not any(credible in source for credible in credible_sources)
    
    def calculate_credibility_score(self, biases, contextual_analysis):
        """Calcule un score de cr√©dibilit√©"""
        base_score = 1.0
        
        # P√©nalit√©s pour les biais
        for bias in biases:
            if "Langage √©motionnel" in bias:
                base_score -= 0.2
            if "Source √† v√©rifier" in bias:
                base_score -= 0.3
            if "Divergence" in bias:
                base_score -= 0.2
        
        # Bonus pour l'urgence et l'impact (sujets importants)
        if contextual_analysis.get('urgence', 0) > 0.5:
            base_score += 0.1
        if contextual_analysis.get('impact', 0) > 0.5:
            base_score += 0.1
        
        return max(0, min(1, base_score))
    
    def generate_bias_recommendations(self, biases):
        """G√©n√®re des recommandations pour corriger les biais"""
        recommendations = []
        
        if "Langage √©motionnel" in str(biases):
            recommendations.append("Recadrer avec un langage plus neutre")
        
        if "Source √† v√©rifier" in str(biases):
            recommendations.append("Recouper avec des sources fiables")
        
        if "Divergence" in str(biases):
            recommendations.append("Contextualiser avec des informations v√©rifi√©es")
        
        return recommendations
    
    def synthesize_analysis(self, article, contextual_analysis, web_research, thematic_analysis, bias_analysis):
        """Synth√©tise toutes les analyses"""
        sentiment = article.get('sentiment', {})
        original_score = sentiment.get('score', 0)
        
        # Calcul du score corrig√© bas√© sur l'analyse approfondie
        corrected_score = self.calculate_corrected_score(
            original_score, 
            contextual_analysis, 
            web_research, 
            bias_analysis
        )
        
        return {
            'score_original': original_score,
            'score_corrected': corrected_score,
            'analyse_contextuelle': contextual_analysis,
            'recherche_web': web_research,
            'analyse_thematique': thematic_analysis,
            'analyse_biases': bias_analysis,
            'confidence': bias_analysis.get('score_credibilite', 0.5),
            'recommandations_globales': self.generate_global_recommendations(
                contextual_analysis, web_research, bias_analysis
            )
        }
    
    def calculate_corrected_score(self, original_score, contextual_analysis, web_research, bias_analysis):
        """Calcule le score corrig√© bas√© sur l'analyse approfondie"""
        correction = 0
        
        # Ajustement bas√© sur l'urgence
        urgency = contextual_analysis.get('urgence', 0)
        if urgency > 0.7:
            correction -= 0.1  # Les sujets urgents sont souvent plus n√©gatifs
        
        # Ajustement bas√© sur les tensions
        if 'g√©opolitique' in contextual_analysis:
            tensions = contextual_analysis.get('tensions', 0)
            if tensions > 0.5:
                correction -= 0.15
        
        # Ajustement bas√© sur la recherche web
        if web_research:
            web_sentiment = web_research.get('sentiment_moyen', 0)
            correction += (web_sentiment - original_score) * 0.3
        
        # Ajustement bas√© sur la cr√©dibilit√©
        credibility = bias_analysis.get('score_credibilite', 0.5)
        credibility_factor = credibility * 2 - 1  # Convertit 0-1 en -1 √† 1
        correction *= credibility_factor
        
        corrected = original_score + correction
        return max(-1, min(1, corrected))
    
    def generate_global_recommendations(self, contextual_analysis, web_research, bias_analysis):
        """G√©n√®re des recommandations globales"""
        recommendations = []
        
        # Recommandations bas√©es sur l'urgence
        if contextual_analysis.get('urgence', 0) > 0.7:
            recommendations.append("üö® SUJET URGENT - Surveillance renforc√©e recommand√©e")
        
        # Recommandations bas√©es sur la port√©e
        scope = contextual_analysis.get('port√©e', 'local')
        if scope == 'international':
            recommendations.append("üåç PORT√âE INTERNATIONALE - Analyse g√©opolitique approfondie")
        
        # Recommandations bas√©es sur la cr√©dibilit√©
        credibility = bias_analysis.get('score_credibilite', 0.5)
        if credibility < 0.7:
            recommendations.append("üîç CR√âDIBILIT√â √Ä V√âRIFIER - Recoupement des sources n√©cessaire")
        
        # Recommandations bas√©es sur la recherche web
        if web_research and web_research.get('coherence', 1) < 0.8:
            recommendations.append("üìä DIVERGENCE CONTEXTUELLE - Analyse comparative recommand√©e")
        
        return recommendations

# Initialiser l'analyseur IA avanc√©
advanced_analyzer = AdvancedIAAnalyzer()

@app.route('/correct_analysis', methods=['POST'])
def correct_analysis():
    """Endpoint pour corriger l'analyse des articles"""
    try:
        data = request.json or {}
        api_key = data.get('apiKey')
        articles = data.get('articles', [])
        current_analysis = data.get('currentAnalysis', {})
        themes = data.get('themes', [])
        
        if not api_key:
            return jsonify({'success': False, 'error': 'Cl√© API requise'})
        
        print(f"üß† Correction de l'analyse pour {len(articles)} articles avec {len(themes)} th√®mes")
        
        # CORRECTION : s'assurer que themes est une liste
        if themes and not isinstance(themes, list):
            themes = [themes]
        
        # Appliquer l'analyse approfondie √† chaque article
        corrected_analyses = []
        for i, article in enumerate(articles):
            print(f"üìù Traitement article {i+1}/{len(articles)}: {article.get('title', '')[:50]}...")
            
            try:
                # Analyse approfondie avec raisonnement
                deep_analysis = advanced_analyzer.perform_deep_analysis(article, themes)
                
                # CORRECTION : s'assurer de la coh√©rence de l'analyse
                final_analysis = ensure_deep_analysis_consistency(deep_analysis, article)
                
                # Calcul de la confiance bas√© sur les features
                confidence = compute_confidence_from_features(final_analysis)
                final_analysis['confidence'] = clamp01(confidence)
                
                corrected_analyses.append(final_analysis)
                
                print(f"‚úÖ Article {i+1} trait√© - Score: {final_analysis.get('score_corrected', 0):.2f}, Confiance: {final_analysis.get('confidence', 0):.2f}")
                
            except Exception as e:
                print(f"‚ùå Erreur traitement article {i+1}: {e}")
                import traceback
                traceback.print_exc()
                
                # En cas d'erreur, utiliser l'analyse de base
                sentiment = article.get('sentiment', {})
                corrected_analyses.append({
                    'score_original': sentiment.get('score', 0),
                    'score_corrected': sentiment.get('score', 0),
                    'confidence': 0.3,
                    'analyse_contextuelle': {},
                    'recherche_web': None,
                    'analyse_thematique': {},
                    'analyse_biases': {'biais_d√©tect√©s': [], 'score_credibilite': 0.5},
                    'recommandations_globales': ['Erreur lors de l\'analyse approfondie']
                })
        
        # CORRECTION : sauvegarde du lot d'analyses
        try:
            save_analysis_batch(corrected_analyses, api_key, themes)
            print(f"üíæ Lot d'analyses sauvegard√© ({len(corrected_analyses)} articles)")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sauvegarde analyses: {e}")
        
        # CORRECTION : corroboration automatique et fusion bay√©sienne
        try:
            print("üîÑ D√©but de la corroboration automatique...")
            corroboration_results = []
            
            for i, (article, analysis) in enumerate(zip(articles, corrected_analyses)):
                try:
                    # Recherche de corroborations pour cet article
                    article_corroborations = find_corroborations(
                        article_title=article.get('title', ''),
                        article_content=article.get('content', ''),
                        themes=themes,
                        api_key=api_key
                    )
                    
                    if article_corroborations:
                        # Appliquer la fusion bay√©sienne si des corroborations trouv√©es
                        from modules.bayesian_fusion import apply_bayesian_fusion
                        
                        fused_analysis = apply_bayesian_fusion(
                            base_analysis=analysis,
                            corroborations=article_corroborations,
                            article_data=article
                        )
                        
                        # Mettre √† jour l'analyse avec les r√©sultats fusionn√©s
                        if fused_analysis:
                            corrected_analyses[i] = fused_analysis
                            print(f"‚úÖ Fusion bay√©sienne appliqu√©e pour l'article {i+1}")
                    
                    corroboration_results.append({
                        'article_index': i,
                        'corroborations_found': len(article_corroborations) if article_corroborations else 0,
                        'corroboration_details': article_corroborations
                    })
                    
                except Exception as e:
                    print(f"‚ùå Erreur corroboration article {i+1}: {e}")
                    import traceback
                    traceback.print_exc()
                    corroboration_results.append({
                        'article_index': i,
                        'corroborations_found': 0,
                        'error': str(e)
                    })
            
            print(f"‚úÖ Corroboration termin√©e: {sum(r.get('corroborations_found', 0) for r in corroboration_results)} corroborations trouv√©es")
            
        except Exception as e:
            print(f"‚ùå Erreur globale dans la corroboration: {e}")
            import traceback
            traceback.print_exc()
            corroboration_results = []
        
        return jsonify({
            'success': True,
            'correctedAnalyses': corrected_analyses,
            'corroborationResults': corroboration_results,
            'summary': {
                'articles_traites': len(corrected_analyses),
                'analyses_corrigees': len([a for a in corrected_analyses if abs(a.get('score_corrected', 0) - a.get('score_original', 0)) > 0.1]),
                'confiance_moyenne': sum(a.get('confidence', 0) for a in corrected_analyses) / len(corrected_analyses) if corrected_analyses else 0
            }
        })
        
    except Exception as e:
        print(f"‚ùå Erreur endpoint correct_analysis: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/generate_report', methods=['POST'])
def generate_report():
    """G√©n√®re un rapport PDF d√©taill√©"""
    try:
        data = request.json or {}
        analyses = data.get('analyses', [])
        themes = data.get('themes', [])
        date_range = data.get('dateRange', {})
        
        if not analyses:
            return jsonify({'success': False, 'error': 'Aucune analyse fournie'})
        
        # Cr√©er le nom du fichier
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rapport_analyse_{timestamp}.pdf"
        filepath = os.path.join(REPORTS_DIR, filename)
        
        # Cr√©er le document PDF
        doc = SimpleDocTemplate(filepath, pagesize=A4)
        styles = getSampleStyleSheet()
        story = []
        
        # Titre
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=16,
            spaceAfter=30,
            alignment=1
        )
        story.append(Paragraph("RAPPORT D'ANALYSE AVANC√âE", title_style))
        
        # M√©tadonn√©es
        meta_style = styles['Normal']
        story.append(Paragraph(f"Date de g√©n√©ration: {datetime.datetime.now().strftime('%d/%m/%Y %H:%M')}", meta_style))
        story.append(Paragraph(f"Nombre d'articles analys√©s: {len(analyses)}", meta_style))
        story.append(Paragraph(f"Th√®mes: {', '.join(str(t) for t in themes)}", meta_style))
        story.append(Spacer(1, 20))
        
        # R√©sum√© statistique
        story.append(Paragraph("R√âSUM√â STATISTIQUE", styles['Heading2']))
        
        # Calculer les statistiques
        original_scores = [a.get('score_original', 0) for a in analyses]
        corrected_scores = [a.get('score_corrected', 0) for a in analyses]
        confidences = [a.get('confidence', 0) for a in analyses]
        
        stats_data = [
            ['M√©trique', 'Valeur'],
            ['Score moyen original', f"{sum(original_scores)/len(original_scores):.3f}"],
            ['Score moyen corrig√©', f"{sum(corrected_scores)/len(corrected_scores):.3f}"],
            ['Confiance moyenne', f"{sum(confidences)/len(confidences):.3f}"],
            ['Corrections significatives', f"{sum(1 for o,c in zip(original_scores, corrected_scores) if abs(o-c) > 0.1)}/{len(analyses)}"]
        ]
        
        stats_table = Table(stats_data, colWidths=[200, 100])
        stats_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 12),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(stats_table)
        story.append(Spacer(1, 20))
        
        # D√©tails par article
        story.append(Paragraph("D√âTAILS PAR ARTICLE", styles['Heading2']))
        
        for i, analysis in enumerate(analyses[:10]):  # Limiter √† 10 articles pour le rapport
            story.append(Paragraph(f"Article {i+1}", styles['Heading3']))
            
            article_data = [
                ['Score original', f"{analysis.get('score_original', 0):.3f}"],
                ['Score corrig√©', f"{analysis.get('score_corrected', 0):.3f}"],
                ['Confiance', f"{analysis.get('confidence', 0):.3f}"],
                ['Biais d√©tect√©s', f"{len(analysis.get('analyse_biases', {}).get('biais_d√©tect√©s', []))}"]
            ]
            
            article_table = Table(article_data, colWidths=[150, 100])
            article_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.lightblue),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(article_table)
            story.append(Spacer(1, 10))
        
        # G√©n√©rer le PDF
        doc.build(story)
        
        return jsonify({
            'success': True,
            'reportUrl': f'/reports/{filename}',
            'filename': filename
        })
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration rapport: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/reports/<filename>')
def download_report(filename):
    """T√©l√©charge un rapport g√©n√©r√©"""
    return send_from_directory(REPORTS_DIR, filename)

@app.route('/health')
def health_check():
    """Endpoint de sant√©"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.datetime.now().isoformat(),
        'reports_count': len(os.listdir(REPORTS_DIR)) if os.path.exists(REPORTS_DIR) else 0
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)